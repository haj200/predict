 

import os
import shutil
from pathlib import Path
import json
import re

def update_scraper_daily(nature_id):
    """
    Met Ã  jour le fichier scraper_daily.py pour une nature donnÃ©e
    """
    scraper_file = Path(f"nature_{nature_id}/scraper/scraper_daily.py")
    
    if not scraper_file.exists():
        print(f"âš ï¸  Fichier non trouvÃ©: {scraper_file}")
        return False
    
    # Lire le contenu actuel
    with open(scraper_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # VÃ©rifier si le nettoyage est dÃ©jÃ  prÃ©sent
    if "convert_montant" in content:
        print(f"âœ… nature_{nature_id}: Nettoyage dÃ©jÃ  prÃ©sent")
        return True
    
    # Ajouter les fonctions de nettoyage aprÃ¨s les imports
    cleaning_functions = '''
def convert_montant(montant_str):
    """Convertit une chaÃ®ne de montant en nombre flottant."""
    if not montant_str:
        return None
    
    # Supprimer les espaces et remplacer la virgule par un point
    montant_str = montant_str.replace(' ', '').replace(',', '.')
    
    # Extraire uniquement les chiffres et le point dÃ©cimal
    match = re.search(r'([\d.]+)', montant_str)
    if not match:
        return None
        
    try:
        return float(match.group(1))
    except ValueError:
        return None

def clean_data(data):
    """Nettoie les donnÃ©es en gardant uniquement les champs requis et en convertissant le montant."""
    cleaned = []
    for item in data:
        if not item:
            continue
            
        cleaned_item = {
            "reference": item.get("reference"),
            "objet": item.get("objet"),
            "acheteur": item.get("acheteur"),
            "montant": convert_montant(item.get("montant"))
        }
        cleaned.append(cleaned_item)
    return cleaned

'''
    
    # Trouver la position aprÃ¨s les imports et avant get_max_page
    lines = content.split('\n')
    insert_position = None
    
    for i, line in enumerate(lines):
        if line.strip().startswith('def get_max_page'):
            insert_position = i
            break
    
    if insert_position is None:
        print(f"âŒ nature_{nature_id}: Impossible de trouver la position d'insertion")
        return False
    
    # InsÃ©rer les fonctions de nettoyage
    lines.insert(insert_position, cleaning_functions)
    
    # Modifier la fonction save_results
    save_results_start = None
    save_results_end = None
    
    for i, line in enumerate(lines):
        if line.strip().startswith('def save_results'):
            save_results_start = i
        elif save_results_start and line.strip().startswith('def '):
            save_results_end = i
            break
    
    if save_results_start is not None:
        # Nouvelle fonction save_results avec nettoyage
        new_save_results = '''def save_results(data, date_str):
    """Sauvegarde les rÃ©sultats par type (attribuÃ©s ou infructueux) et les donnÃ©es nettoyÃ©es."""
    
    attribues = [d for d in data if d and d['attribue']]
    
    # Sauvegarder les donnÃ©es brutes
    if attribues:
        path = os.path.join(DAILY_DIR, f"attributed_day.json")
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(attribues, f, ensure_ascii=False, indent=2)
        print(f"âœ… {len(attribues)} consultations attribuÃ©es sauvegardÃ©es dans {path}")
    
    # Nettoyer et sauvegarder les donnÃ©es nettoyÃ©es
    if attribues:
        cleaned_data = clean_data(attribues)
        cleaned_path = os.path.join(DAILY_DIR, f"attributed_cleaned_day.json")
        with open(cleaned_path, 'w', encoding='utf-8') as f:
            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ§¹ {len(cleaned_data)} donnÃ©es nettoyÃ©es sauvegardÃ©es dans {cleaned_path}")
'''
        
        # Remplacer la fonction save_results
        if save_results_end:
            lines[save_results_start:save_results_end] = new_save_results.split('\n')
        else:
            # Si on ne trouve pas la fin, remplacer jusqu'Ã  la fin
            lines[save_results_start:] = new_save_results.split('\n')
    
    # Modifier la fonction main pour ajouter le message de nettoyage
    for i, line in enumerate(lines):
        if "print('ğŸ‰ Extraction terminÃ©e.')" in line:
            lines[i] = "    print('ğŸ‰ Extraction et nettoyage terminÃ©s.')"
        elif "print(all_data)" in line:
            lines[i] = "    print(f'ğŸ“Š Total des donnÃ©es extraites : {len(all_data)}')"
    
    # Ã‰crire le nouveau contenu
    new_content = '\n'.join(lines)
    
    # Sauvegarder le fichier modifiÃ©
    with open(scraper_file, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    print(f"âœ… nature_{nature_id}: Fichier mis Ã  jour avec succÃ¨s")
    return True

def main():
    """
    Met Ã  jour tous les scraper_daily.py
    """
    base_dir = Path(".")
    updated_count = 0
    total_count = 0
    
    print("ğŸ”„ Mise Ã  jour des scraper_daily.py...")
    
    # Parcourir tous les dossiers nature_
    for item in base_dir.iterdir():
        if item.is_dir() and item.name.startswith("nature_"):
            nature_id = item.name.replace("nature_", "")
            total_count += 1
            
            if update_scraper_daily(nature_id):
                updated_count += 1
    
    print(f"\nğŸ“Š RÃ‰SUMÃ‰:")
    print(f"   ğŸ“ Total des natures: {total_count}")
    print(f"   âœ… Fichiers mis Ã  jour: {updated_count}")
    print(f"   âš ï¸  Fichiers non modifiÃ©s: {total_count - updated_count}")

if __name__ == "__main__":
    main() 