

import json
import spacy
from unidecode import unidecode
from pathlib import Path
from collections import defaultdict, Counter
import math

# Charger le modÃ¨le spaCy pour le franÃ§ais
try:
    nlp = spacy.load('fr_core_news_sm')
except OSError:
    print("âŒ ModÃ¨le spaCy 'fr_core_news_sm' non trouvÃ©. Installation requise:")
    print("python -m spacy download fr_core_news_sm")
    exit(1)

# Mots vides Ã  exclure
stop_words = set(nlp.Defaults.stop_words)

def normalize_text(text):
    """Minuscule + suppression des accents"""
    if not text:
        return ""
    text = text.lower()
    text = unidecode(text)
    return text

def process_text(text):
    """Traite un texte avec spaCy et retourne les mots lemmatisÃ©s"""
    if not text:
        return []
    text = normalize_text(text)
    doc = nlp(text)
    
    # Lemmatisation + filtrage
    words = [token.lemma_ for token in doc
             if token.lemma_ not in stop_words
             and token.is_alpha
             and len(token.lemma_) > 1]
    
    return words

def categorize_by_amount(nature_id, step=2000):
    """
    CatÃ©gorise les donnÃ©es par montant et extrait les mots significatifs
    """
    data_file = Path(f"nature_{nature_id}/data/attributed_cleaned.json")
    
    if not data_file.exists():
        print(f"âŒ Fichier non trouvÃ©: {data_file}")
        return None
    
    # Charger les donnÃ©es
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if not data:
        print(f"âš ï¸  Aucune donnÃ©e trouvÃ©e pour nature_{nature_id}")
        return None
    
    # Trouver le montant maximum
    max_amount = max(item.get('montant', 0) for item in data)
    
    # CrÃ©er les catÃ©gories de montant
    categories = defaultdict(list)
    
    for item in data:
        montant = item.get('montant', 0)
        if montant is None:
            continue
            
        # Calculer la catÃ©gorie
        category_start = math.floor(montant / step) * step
        category_end = category_start + step
        category_key = f"{category_start}-{category_end}"
        
        # Combiner les champs texte
        combined_text = f"{item.get('acheteur', '')} {item.get('reference', '')} {item.get('objet', '')}"
        
        categories[category_key].append({
            'montant': montant,
            'text': combined_text
        })
    
    # Traiter chaque catÃ©gorie
    results = {}
    
    for category, items in categories.items():
        if not items:  # Ignorer les catÃ©gories vides
            continue
            
        # Combiner tous les textes de la catÃ©gorie
        all_text = " ".join(item['text'] for item in items)
        
        # Extraire les mots significatifs
        words = process_text(all_text)
        
        # Compter les occurrences
        word_counts = Counter(words)
        
        # Prendre les mots les plus frÃ©quents (top 20)
        top_words = word_counts.most_common(20)
        
        results[category] = {
            'count': len(items),
            'min_amount': min(item['montant'] for item in items),
            'max_amount': max(item['montant'] for item in items),
            'words': dict(top_words)
        }
    
    return results

def process_all_natures():
    """
    Traite toutes les natures et sauvegarde les rÃ©sultats
    """
    base_dir = Path(".")
    all_results = {}
    
    print("ğŸ” Traitement des donnÃ©es par montant...")
    
    # Parcourir tous les dossiers nature_
    for item in base_dir.iterdir():
        if item.is_dir() and item.name.startswith("nature_"):
            nature_id = item.name.replace("nature_", "")
            
            print(f"ğŸ“ Traitement de nature_{nature_id}...")
            
            results = categorize_by_amount(nature_id)
            if results:
                all_results[nature_id] = results
                print(f"   âœ… {len(results)} catÃ©gories crÃ©Ã©es")
            else:
                print(f"   âš ï¸  Aucun rÃ©sultat pour nature_{nature_id}")
    
    # Sauvegarder les rÃ©sultats
    output_file = "categorization_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… RÃ©sultats sauvegardÃ©s dans {output_file}")
    
    # Afficher un rÃ©sumÃ©
    total_categories = sum(len(results) for results in all_results.values())
    total_natures = len(all_results)
    
    print(f"\nğŸ“Š RÃ‰SUMÃ‰:")
    print(f"   ğŸ“ Natures traitÃ©es: {total_natures}")
    print(f"   ğŸ“Š CatÃ©gories crÃ©Ã©es: {total_categories}")
    
    return all_results

if __name__ == "__main__":
    process_all_natures() 